{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db583467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:36:18.780817Z",
     "iopub.status.busy": "2023-11-08T08:36:18.780479Z",
     "iopub.status.idle": "2023-11-08T08:36:18.792238Z",
     "shell.execute_reply": "2023-11-08T08:36:18.791411Z"
    },
    "papermill": {
     "duration": 0.020668,
     "end_time": "2023-11-08T08:36:18.794239",
     "exception": false,
     "start_time": "2023-11-08T08:36:18.773571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08962272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:36:18.806131Z",
     "iopub.status.busy": "2023-11-08T08:36:18.805875Z",
     "iopub.status.idle": "2023-11-08T08:36:19.757028Z",
     "shell.execute_reply": "2023-11-08T08:36:19.755837Z"
    },
    "papermill": {
     "duration": 0.959543,
     "end_time": "2023-11-08T08:36:19.759451",
     "exception": false,
     "start_time": "2023-11-08T08:36:18.799908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-56454a6a-ddf4-5902-1f2b-fde86244ed1c)\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9811951a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:36:19.771926Z",
     "iopub.status.busy": "2023-11-08T08:36:19.771609Z",
     "iopub.status.idle": "2023-11-08T08:37:45.010193Z",
     "shell.execute_reply": "2023-11-08T08:37:45.009086Z"
    },
    "papermill": {
     "duration": 85.252134,
     "end_time": "2023-11-08T08:37:45.017083",
     "exception": false,
     "start_time": "2023-11-08T08:36:19.764949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\r\n",
      "cmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 6.0.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\r\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "google-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\r\n",
      "jupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.0.1 requires kubernetes<27,>=8.0.0, but you have kubernetes 28.1.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\r\n",
      "ray 2.5.1 requires grpcio<=1.51.3,>=1.42.0; python_version >= \"3.10\" and sys_platform != \"darwin\", but you have grpcio 1.59.2 which is incompatible.\r\n",
      "yapf 0.40.1 requires importlib-metadata>=6.6.0, but you have importlib-metadata 6.0.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "chromadb 0.4.16 requires grpcio>=1.58.0, but you have grpcio 1.51.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCPU times: user 946 ms, sys: 251 ms, total: 1.2 s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! pip install -qq -U langchain tiktoken pypdf chromadb faiss-gpu\n",
    "! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n",
    "! pip install -qq -U bitsandbytes bitsandbytes-cuda117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449d1513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:37:45.029481Z",
     "iopub.status.busy": "2023-11-08T08:37:45.029136Z",
     "iopub.status.idle": "2023-11-08T08:37:59.977162Z",
     "shell.execute_reply": "2023-11-08T08:37:59.976181Z"
    },
    "papermill": {
     "duration": 14.95688,
     "end_time": "2023-11-08T08:37:59.979497",
     "exception": false,
     "start_time": "2023-11-08T08:37:45.022617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.331\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import textwrap\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "\n",
    "#quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "#os.environ['TRANSFORMERS_CACHE'] = 'cache/'\n",
    "\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e16edb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:37:59.992180Z",
     "iopub.status.busy": "2023-11-08T08:37:59.991846Z",
     "iopub.status.idle": "2023-11-08T08:38:02.462224Z",
     "shell.execute_reply": "2023-11-08T08:38:02.461323Z"
    },
    "papermill": {
     "duration": 2.47948,
     "end_time": "2023-11-08T08:38:02.464858",
     "exception": false,
     "start_time": "2023-11-08T08:37:59.985378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Multi-document retriever\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA, VectorDBQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3bbc57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:38:02.479856Z",
     "iopub.status.busy": "2023-11-08T08:38:02.478583Z",
     "iopub.status.idle": "2023-11-08T08:38:04.968293Z",
     "shell.execute_reply": "2023-11-08T08:38:04.967267Z"
    },
    "papermill": {
     "duration": 2.499184,
     "end_time": "2023-11-08T08:38:04.970298",
     "exception": false,
     "start_time": "2023-11-08T08:38:02.471114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading model:  \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbfd6aca60c45709f2d39830e3852f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'YanaS/llama-2-7b-langchain-chat-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'YanaS/llama-2-7b-langchain-chat-GGUF' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:786\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2008\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2002\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2003\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2005\u001b[0m     )\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2009\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2011\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2012\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2013\u001b[0m     )\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'YanaS/llama-2-7b-langchain-chat-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'YanaS/llama-2-7b-langchain-chat-GGUF' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('\\nDownloading model: ', '\\n\\n')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"YanaS/llama-2-7b-langchain-chat-GGUF\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"YanaS/llama-2-7b-langchain-chat-GGUF\",\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             #cache_dir=\"cache/\",\n",
    "                                             #offload_folder=\"save_folder\"\n",
    "#                                            rope_scaling = {'type': 'dynamic', 'factor': 2.0}\n",
    "                                            )\n",
    "max_len = 1024 # 512\n",
    "task = \"text-generation\"\n",
    "T = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03979642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:38:04.984413Z",
     "iopub.status.busy": "2023-11-08T08:38:04.984096Z",
     "iopub.status.idle": "2023-11-08T08:38:05.178938Z",
     "shell.execute_reply": "2023-11-08T08:38:05.177660Z"
    },
    "papermill": {
     "duration": 0.203667,
     "end_time": "2023-11-08T08:38:05.180776",
     "exception": true,
     "start_time": "2023-11-08T08:38:04.977109",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m----> 2\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[43mtask\u001b[49m,\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[1;32m      5\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_len,\n\u001b[1;32m      6\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m      7\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m      8\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.15\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mpipe)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'task' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=task,\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=max_len,\n",
    "    temperature=T,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca1c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:01:09.353914Z",
     "iopub.status.busy": "2023-11-04T05:01:09.353545Z",
     "iopub.status.idle": "2023-11-04T05:01:09.367052Z",
     "shell.execute_reply": "2023-11-04T05:01:09.366262Z",
     "shell.execute_reply.started": "2023-11-04T05:01:09.35388Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc8fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:01:09.368699Z",
     "iopub.status.busy": "2023-11-04T05:01:09.368362Z",
     "iopub.status.idle": "2023-11-04T05:03:18.047512Z",
     "shell.execute_reply": "2023-11-04T05:03:18.046594Z",
     "shell.execute_reply.started": "2023-11-04T05:01:09.368666Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.copytree('/kaggle/input/mkvc-11-finalmix/kaggle/working/mk-vectordb-chroma-eleven', '/kaggle/working/mk-vectordb-chroma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e378703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:18.049283Z",
     "iopub.status.busy": "2023-11-04T05:03:18.048895Z",
     "iopub.status.idle": "2023-11-04T05:03:39.991258Z",
     "shell.execute_reply": "2023-11-04T05:03:39.990211Z",
     "shell.execute_reply.started": "2023-11-04T05:03:18.04925Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"LazarusNLP/congen-indobert-base\", model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "vectordb = Chroma(persist_directory='mk-vectordb-chroma',embedding_function=instructor_embeddings,)\n",
    "#vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c264f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:39.99538Z",
     "iopub.status.busy": "2023-11-04T05:03:39.995057Z",
     "iopub.status.idle": "2023-11-04T05:03:40.002326Z",
     "shell.execute_reply": "2023-11-04T05:03:40.001187Z",
     "shell.execute_reply.started": "2023-11-04T05:03:39.995353Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3, \"search_type\" : \"similarity\"})\n",
    "#retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 6, \"lambda_mult\": 0.25})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                       chain_type=\"stuff\", \n",
    "                                       retriever=retriever, \n",
    "                                       return_source_documents=True,\n",
    "                                       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a77ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:40.003567Z",
     "iopub.status.busy": "2023-11-04T05:03:40.003308Z",
     "iopub.status.idle": "2023-11-04T05:03:40.019709Z",
     "shell.execute_reply": "2023-11-04T05:03:40.01871Z",
     "shell.execute_reply.started": "2023-11-04T05:03:40.003544Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acdee4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:40.021118Z",
     "iopub.status.busy": "2023-11-04T05:03:40.02084Z",
     "iopub.status.idle": "2023-11-04T05:03:40.028477Z",
     "shell.execute_reply": "2023-11-04T05:03:40.027722Z",
     "shell.execute_reply.started": "2023-11-04T05:03:40.021093Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e23ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:40.029789Z",
     "iopub.status.busy": "2023-11-04T05:03:40.029539Z",
     "iopub.status.idle": "2023-11-04T05:03:40.037575Z",
     "shell.execute_reply": "2023-11-04T05:03:40.036706Z",
     "shell.execute_reply.started": "2023-11-04T05:03:40.029767Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_ans(query):\n",
    "    llm_response = qa_chain(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2858169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:40.038926Z",
     "iopub.status.busy": "2023-11-04T05:03:40.038662Z",
     "iopub.status.idle": "2023-11-04T05:03:40.051631Z",
     "shell.execute_reply": "2023-11-04T05:03:40.050701Z",
     "shell.execute_reply.started": "2023-11-04T05:03:40.038903Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118bde3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:40.053399Z",
     "iopub.status.busy": "2023-11-04T05:03:40.052909Z",
     "iopub.status.idle": "2023-11-04T05:03:40.113442Z",
     "shell.execute_reply": "2023-11-04T05:03:40.112517Z",
     "shell.execute_reply.started": "2023-11-04T05:03:40.053365Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/kaggle/input/mkri-qa-3/dataset.csv',sep=';')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ba67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T05:03:40.114881Z",
     "iopub.status.busy": "2023-11-04T05:03:40.114602Z",
     "iopub.status.idle": "2023-11-04T07:00:47.976268Z",
     "shell.execute_reply": "2023-11-04T07:00:47.975373Z",
     "shell.execute_reply.started": "2023-11-04T05:03:40.114856Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for x,y in zip(dataset.no,dataset.qa_comb):\n",
    "    llm_time_start = time.time()\n",
    "    print(\"Nomor: \",x)\n",
    "    print(y)\n",
    "    llm_ans(y)\n",
    "    llm_time_end = time.time()\n",
    "    print(\"Time: \",llm_time_end - llm_time_start)\n",
    "    print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50641709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T07:00:47.977872Z",
     "iopub.status.busy": "2023-11-04T07:00:47.97759Z",
     "iopub.status.idle": "2023-11-04T07:00:47.982419Z",
     "shell.execute_reply": "2023-11-04T07:00:47.981371Z",
     "shell.execute_reply.started": "2023-11-04T07:00:47.977846Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 112.531083,
   "end_time": "2023-11-08T08:38:07.979102",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-08T08:36:15.448019",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0b7a35a76cc546a89ee5cfb4b52df929": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27a65296035e4f78a1f5703c81a507fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_aa38260505924c738350e1c292a52ad8",
       "max": 632.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5e510c3ef59c4b858c450289c8c920b8",
       "value": 632.0
      }
     },
     "30fe3cb6639741fd85fe82339242cd63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "54fcf312c15c443dbbaac5ac7f70477b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e510c3ef59c4b858c450289c8c920b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6cbfd6aca60c45709f2d39830e3852f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_84fe070ea33c4fdca170fa7351ecc1db",
        "IPY_MODEL_27a65296035e4f78a1f5703c81a507fd",
        "IPY_MODEL_88e91880bae140a984a57ea4ac1c3c92"
       ],
       "layout": "IPY_MODEL_0b7a35a76cc546a89ee5cfb4b52df929"
      }
     },
     "84fe070ea33c4fdca170fa7351ecc1db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e53517e7ff4c41a98c82622c13db5fa5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_30fe3cb6639741fd85fe82339242cd63",
       "value": "Downloading (â€¦)lve/main/config.json: 100%"
      }
     },
     "88e91880bae140a984a57ea4ac1c3c92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_54fcf312c15c443dbbaac5ac7f70477b",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ebe7bf19e7cf47499678bcc0e0b87d15",
       "value": " 632/632 [00:00&lt;00:00, 49.4kB/s]"
      }
     },
     "aa38260505924c738350e1c292a52ad8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e53517e7ff4c41a98c82622c13db5fa5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebe7bf19e7cf47499678bcc0e0b87d15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
