{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6867748,"sourceType":"datasetVersion","datasetId":3946908},{"sourceId":6606645,"sourceType":"datasetVersion","datasetId":3811798}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/code/rkuo2000/chromadb-langchain-gpt4all\n\nhttps://python.langchain.com/docs/integrations/llms/gpt4all\n\nhttps://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/python\n\nhttps://www.kaggle.com/code/gpreda/test-gpt4all-on-kaggle","metadata":{}},{"cell_type":"code","source":"import time\nstart = time.time()","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:20:18.21175Z","iopub.execute_input":"2023-12-14T08:20:18.212032Z","iopub.status.idle":"2023-12-14T08:20:18.222866Z","shell.execute_reply.started":"2023-12-14T08:20:18.212006Z","shell.execute_reply":"2023-12-14T08:20:18.221841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! nvidia-smi -L","metadata":{"papermill":{"duration":1.003378,"end_time":"2023-08-14T08:18:11.186896","exception":false,"start_time":"2023-08-14T08:18:10.183518","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:20:18.224436Z","iopub.execute_input":"2023-12-14T08:20:18.224747Z","iopub.status.idle":"2023-12-14T08:20:19.254763Z","shell.execute_reply.started":"2023-12-14T08:20:18.224722Z","shell.execute_reply":"2023-12-14T08:20:19.253684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n! pip install -qq -U langchain tiktoken pypdf chromadb faiss-gpu\n! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n! pip install -qq -U bitsandbytes bitsandbytes-cuda117\n! pip install -qq -U gpt4all","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:20:19.256843Z","iopub.execute_input":"2023-12-14T08:20:19.257166Z","iopub.status.idle":"2023-12-14T08:22:28.865359Z","shell.execute_reply.started":"2023-12-14T08:20:19.257136Z","shell.execute_reply":"2023-12-14T08:22:28.864218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!wget https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin models","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:22:28.866997Z","iopub.execute_input":"2023-12-14T08:22:28.867338Z","iopub.status.idle":"2023-12-14T08:22:28.872102Z","shell.execute_reply.started":"2023-12-14T08:22:28.867305Z","shell.execute_reply":"2023-12-14T08:22:28.871161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport textwrap\nimport shutil\nimport pandas as pd\nimport glob\n\nimport langchain\nfrom langchain.llms import HuggingFacePipeline\n\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n\n#quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n\n#os.environ['TRANSFORMERS_CACHE'] = 'cache/'\n\nprint(langchain.__version__)","metadata":{"papermill":{"duration":20.206704,"end_time":"2023-08-14T08:22:59.802446","exception":false,"start_time":"2023-08-14T08:22:39.595742","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:22:28.87481Z","iopub.execute_input":"2023-12-14T08:22:28.875396Z","iopub.status.idle":"2023-12-14T08:22:45.379905Z","shell.execute_reply.started":"2023-12-14T08:22:28.87537Z","shell.execute_reply":"2023-12-14T08:22:45.378859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Multi-document retriever\nfrom langchain.vectorstores import Chroma, FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nfrom langchain.chains import RetrievalQA, VectorDBQA\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import DirectoryLoader\n#from gpt4all import GPT4All\nfrom langchain.llms import GPT4All\nfrom langchain.prompts import PromptTemplate\n\n\nfrom InstructorEmbedding import INSTRUCTOR\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings","metadata":{"papermill":{"duration":0.709403,"end_time":"2023-08-14T08:23:00.521487","exception":false,"start_time":"2023-08-14T08:22:59.812084","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:22:45.381137Z","iopub.execute_input":"2023-12-14T08:22:45.381426Z","iopub.status.idle":"2023-12-14T08:22:48.379756Z","shell.execute_reply.started":"2023-12-14T08:22:45.3814Z","shell.execute_reply":"2023-12-14T08:22:48.378908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://gpt4all.io/models/gguf/mpt-7b-chat-merges-q4_0.gguf","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:22:48.381093Z","iopub.execute_input":"2023-12-14T08:22:48.382052Z","iopub.status.idle":"2023-12-14T08:23:35.369882Z","shell.execute_reply.started":"2023-12-14T08:22:48.382012Z","shell.execute_reply":"2023-12-14T08:23:35.368729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nllm = GPT4All(model=\"/kaggle/working/mpt-7b-chat-merges-q4_0.gguf\")","metadata":{"papermill":{"duration":0.794861,"end_time":"2023-08-14T08:34:11.639896","exception":false,"start_time":"2023-08-14T08:34:10.845035","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:26:03.609733Z","iopub.execute_input":"2023-12-14T08:26:03.610121Z","iopub.status.idle":"2023-12-14T08:26:03.682987Z","shell.execute_reply.started":"2023-12-14T08:26:03.610088Z","shell.execute_reply":"2023-12-14T08:26:03.681796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(llm.generate(\"The capital of France is \", max_tokens=1028))","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:23:36.732467Z","iopub.status.idle":"2023-12-14T08:23:36.732877Z","shell.execute_reply.started":"2023-12-14T08:23:36.732684Z","shell.execute_reply":"2023-12-14T08:23:36.732711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm","metadata":{"papermill":{"duration":0.022732,"end_time":"2023-08-14T08:34:11.676012","exception":false,"start_time":"2023-08-14T08:34:11.65328","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:23:36.735303Z","iopub.status.idle":"2023-12-14T08:23:36.735667Z","shell.execute_reply.started":"2023-12-14T08:23:36.735479Z","shell.execute_reply":"2023-12-14T08:23:36.735496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.copytree('/kaggle/input/mkvc-7-finalmix/kaggle/working/mk-vectordb-chroma-seven', '/kaggle/working/mk-vectordb-chroma')","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:23:36.736804Z","iopub.status.idle":"2023-12-14T08:23:36.737258Z","shell.execute_reply.started":"2023-12-14T08:23:36.737031Z","shell.execute_reply":"2023-12-14T08:23:36.737052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ninstructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"intfloat/multilingual-e5-small\", model_kwargs={\"device\": \"cuda\"})\n\nvectordb = Chroma(persist_directory='mk-vectordb-chroma',embedding_function=instructor_embeddings,)\n#vectordb.persist()","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:23:36.739092Z","iopub.status.idle":"2023-12-14T08:23:36.739422Z","shell.execute_reply.started":"2023-12-14T08:23:36.739262Z","shell.execute_reply":"2023-12-14T08:23:36.739278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nretriever = vectordb.as_retriever(search_kwargs={\"k\": 3, \"search_type\" : \"similarity\"})\n#retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 6, \"lambda_mult\": 0.25})\n\nqa_chain = RetrievalQA.from_chain_type(llm=llm, \n                                       chain_type=\"stuff\", \n                                       retriever=retriever, \n                                       return_source_documents=True,\n                                       verbose=False)","metadata":{"papermill":{"duration":0.196303,"end_time":"2023-08-14T15:23:42.267186","exception":false,"start_time":"2023-08-14T15:23:42.070883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:23:36.740308Z","iopub.status.idle":"2023-12-14T08:23:36.740712Z","shell.execute_reply.started":"2023-12-14T08:23:36.740492Z","shell.execute_reply":"2023-12-14T08:23:36.740512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa_chain","metadata":{"papermill":{"duration":0.184591,"end_time":"2023-08-14T15:23:42.614585","exception":false,"start_time":"2023-08-14T15:23:42.429994","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:23:36.741898Z","iopub.status.idle":"2023-12-14T08:23:36.742214Z","shell.execute_reply.started":"2023-12-14T08:23:36.742058Z","shell.execute_reply":"2023-12-14T08:23:36.742073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wrap_text_preserve_newlines(text, width=110):\n    # Split the input text into lines based on newline characters\n    lines = text.split('\\n')\n\n    # Wrap each line individually\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n\n    # Join the wrapped lines back together using newline characters\n    wrapped_text = '\\n'.join(wrapped_lines)\n\n    return wrapped_text\n\ndef process_llm_response(llm_response):\n    print(wrap_text_preserve_newlines(llm_response['result']))\n    print('\\n\\nSources:')\n    for source in llm_response[\"source_documents\"]:\n        print(source.metadata['source'])","metadata":{"papermill":{"duration":0.188085,"end_time":"2023-08-14T15:23:42.9729","exception":false,"start_time":"2023-08-14T15:23:42.784815","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:23:36.743376Z","iopub.status.idle":"2023-12-14T08:23:36.743723Z","shell.execute_reply.started":"2023-12-14T08:23:36.743536Z","shell.execute_reply":"2023-12-14T08:23:36.743552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def llm_ans(query):\n    llm_response = qa_chain(query)\n    ans = process_llm_response(llm_response)\n    return ans","metadata":{"papermill":{"duration":0.177637,"end_time":"2023-08-14T15:23:43.32095","exception":false,"start_time":"2023-08-14T15:23:43.143313","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:23:36.744962Z","iopub.status.idle":"2023-12-14T08:23:36.745346Z","shell.execute_reply.started":"2023-12-14T08:23:36.74517Z","shell.execute_reply":"2023-12-14T08:23:36.745186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model","metadata":{"papermill":{"duration":0.181886,"end_time":"2023-08-14T15:23:44.126656","exception":false,"start_time":"2023-08-14T15:23:43.94477","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-14T08:23:36.746833Z","iopub.status.idle":"2023-12-14T08:23:36.747157Z","shell.execute_reply.started":"2023-12-14T08:23:36.746997Z","shell.execute_reply":"2023-12-14T08:23:36.747013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/mkri-qa-3/dataset.csv',sep=';')\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:23:36.748254Z","iopub.status.idle":"2023-12-14T08:23:36.748655Z","shell.execute_reply.started":"2023-12-14T08:23:36.748465Z","shell.execute_reply":"2023-12-14T08:23:36.748483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor x,y in zip(dataset.no,dataset.qa_comb):\n    llm_time_start = time.time()\n    print(\"Nomor: \",x)\n    print(y)\n    llm_ans(y)\n    llm_time_end = time.time()\n    print(\"Time: \",llm_time_end - llm_time_start)\n    print(\"==============================================\")","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:23:36.750149Z","iopub.status.idle":"2023-12-14T08:23:36.750516Z","shell.execute_reply.started":"2023-12-14T08:23:36.750342Z","shell.execute_reply":"2023-12-14T08:23:36.750358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end = time.time()\nprint(end - start)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T08:23:36.751865Z","iopub.status.idle":"2023-12-14T08:23:36.752181Z","shell.execute_reply.started":"2023-12-14T08:23:36.752023Z","shell.execute_reply":"2023-12-14T08:23:36.752039Z"},"trusted":true},"execution_count":null,"outputs":[]}]}