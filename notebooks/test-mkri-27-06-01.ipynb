{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca9f147",
   "metadata": {
    "papermill": {
     "duration": 0.004818,
     "end_time": "2024-01-17T11:40:14.024800",
     "exception": false,
     "start_time": "2024-01-17T11:40:14.019982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## https://www.kaggle.com/code/rkuo2000/chromadb-langchain-gpt4all\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/gpt4all\n",
    "\n",
    "https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/python\n",
    "\n",
    "https://www.kaggle.com/code/gpreda/test-gpt4all-on-kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2aa2fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:40:14.036263Z",
     "iopub.status.busy": "2024-01-17T11:40:14.035129Z",
     "iopub.status.idle": "2024-01-17T11:40:14.046581Z",
     "shell.execute_reply": "2024-01-17T11:40:14.045375Z"
    },
    "papermill": {
     "duration": 0.020141,
     "end_time": "2024-01-17T11:40:14.049545",
     "exception": false,
     "start_time": "2024-01-17T11:40:14.029404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf70b3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:40:14.060199Z",
     "iopub.status.busy": "2024-01-17T11:40:14.059804Z",
     "iopub.status.idle": "2024-01-17T11:40:15.117731Z",
     "shell.execute_reply": "2024-01-17T11:40:15.116385Z"
    },
    "papermill": {
     "duration": 1.066766,
     "end_time": "2024-01-17T11:40:15.120893",
     "exception": false,
     "start_time": "2024-01-17T11:40:14.054127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08da2cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:40:15.137095Z",
     "iopub.status.busy": "2024-01-17T11:40:15.136699Z",
     "iopub.status.idle": "2024-01-17T11:42:14.608483Z",
     "shell.execute_reply": "2024-01-17T11:42:14.606862Z"
    },
    "papermill": {
     "duration": 119.487417,
     "end_time": "2024-01-17T11:42:14.615417",
     "exception": false,
     "start_time": "2024-01-17T11:40:15.128000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n",
      "google-cloud-pubsublite 1.8.3 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\r\n",
      "jupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.0.1 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "opentelemetry-exporter-otlp 1.19.0 requires opentelemetry-exporter-otlp-proto-grpc==1.19.0, but you have opentelemetry-exporter-otlp-proto-grpc 1.22.0 which is incompatible.\r\n",
      "opentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-exporter-otlp-proto-common==1.19.0, but you have opentelemetry-exporter-otlp-proto-common 1.22.0 which is incompatible.\r\n",
      "opentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-proto==1.19.0, but you have opentelemetry-proto 1.22.0 which is incompatible.\r\n",
      "opentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-sdk~=1.19.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCPU times: user 2.1 s, sys: 342 ms, total: 2.44 s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! pip install -qq -U langchain tiktoken pypdf chromadb faiss-gpu\n",
    "! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n",
    "! pip install -qq -U bitsandbytes bitsandbytes-cuda117\n",
    "! pip install -qq -U gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96ece55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:42:14.627891Z",
     "iopub.status.busy": "2024-01-17T11:42:14.627463Z",
     "iopub.status.idle": "2024-01-17T11:42:14.632990Z",
     "shell.execute_reply": "2024-01-17T11:42:14.631688Z"
    },
    "papermill": {
     "duration": 0.015197,
     "end_time": "2024-01-17T11:42:14.635406",
     "exception": false,
     "start_time": "2024-01-17T11:42:14.620209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6839084b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:42:14.648998Z",
     "iopub.status.busy": "2024-01-17T11:42:14.647776Z",
     "iopub.status.idle": "2024-01-17T11:42:36.183037Z",
     "shell.execute_reply": "2024-01-17T11:42:36.181837Z"
    },
    "papermill": {
     "duration": 21.544897,
     "end_time": "2024-01-17T11:42:36.185647",
     "exception": false,
     "start_time": "2024-01-17T11:42:14.640750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import textwrap\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "\n",
    "#quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "#os.environ['TRANSFORMERS_CACHE'] = 'cache/'\n",
    "\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253854b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:42:36.197984Z",
     "iopub.status.busy": "2024-01-17T11:42:36.197200Z",
     "iopub.status.idle": "2024-01-17T11:42:39.392159Z",
     "shell.execute_reply": "2024-01-17T11:42:39.390926Z"
    },
    "papermill": {
     "duration": 3.20406,
     "end_time": "2024-01-17T11:42:39.394879",
     "exception": false,
     "start_time": "2024-01-17T11:42:36.190819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Multi-document retriever\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA, VectorDBQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "#from gpt4all import GPT4All\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96024c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:42:39.407957Z",
     "iopub.status.busy": "2024-01-17T11:42:39.406996Z",
     "iopub.status.idle": "2024-01-17T11:46:46.725177Z",
     "shell.execute_reply": "2024-01-17T11:46:46.722838Z"
    },
    "papermill": {
     "duration": 247.327998,
     "end_time": "2024-01-17T11:46:46.728537",
     "exception": false,
     "start_time": "2024-01-17T11:42:39.400539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-17 11:42:40--  https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf\r\n",
      "Resolving gpt4all.io (gpt4all.io)... 104.26.0.159, 172.67.71.169, 104.26.1.159, ...\r\n",
      "Connecting to gpt4all.io (gpt4all.io)|104.26.0.159|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 7365856064 (6.9G)\r\n",
      "Saving to: 'orca-2-13b.Q4_0.gguf'\r\n",
      "\r\n",
      "orca-2-13b.Q4_0.ggu  97%[==================> ]   6.69G  27.6MB/s    in 3m 57s  \r\n",
      "\r\n",
      "2024-01-17 11:46:38 (28.9 MB/s) - Connection closed at byte 7188709376. Retrying.\r\n",
      "\r\n",
      "--2024-01-17 11:46:39--  (try: 2)  https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf\r\n",
      "Connecting to gpt4all.io (gpt4all.io)|104.26.0.159|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 206 Partial Content\r\n",
      "Length: 7365856064 (6.9G), 177146688 (169M) remaining\r\n",
      "Saving to: 'orca-2-13b.Q4_0.gguf'\r\n",
      "\r\n",
      "orca-2-13b.Q4_0.ggu 100%[+++++++++++++++++++>]   6.86G  31.3MB/s    in 5.9s    \r\n",
      "\r\n",
      "2024-01-17 11:46:46 (28.6 MB/s) - 'orca-2-13b.Q4_0.gguf' saved [7365856064/7365856064]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5431c051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T11:46:46.871596Z",
     "iopub.status.busy": "2024-01-17T11:46:46.871062Z",
     "iopub.status.idle": "2024-01-17T11:46:48.319163Z",
     "shell.execute_reply": "2024-01-17T11:46:48.317175Z"
    },
    "papermill": {
     "duration": 1.522753,
     "end_time": "2024-01-17T11:46:48.321377",
     "exception": true,
     "start_time": "2024-01-17T11:46:46.798624",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /opt/conda/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/orca-2-13b.Q4_0.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/llms/gpt4all.py:132\u001b[0m, in \u001b[0;36mGPT4All.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate that the python package exists in the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT4All \u001b[38;5;28;01mas\u001b[39;00m GPT4AllModel\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import gpt4all python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install gpt4all`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gpt4all/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embed4All \u001b[38;5;28;01mas\u001b[39;00m Embed4All, GPT4All \u001b[38;5;28;01mas\u001b[39;00m GPT4All\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyllmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLModel \u001b[38;5;28;01mas\u001b[39;00m LLModel\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gpt4all/gpt4all.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IncompleteRead, ProtocolError\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyllmodel\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# TODO: move to config\u001b[39;00m\n\u001b[1;32m     21\u001b[0m DEFAULT_MODEL_DIRECTORY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(Path\u001b[38;5;241m.\u001b[39mhome()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4all\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gpt4all/pyllmodel.py:38\u001b[0m\n\u001b[1;32m     33\u001b[0m         lib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(\u001b[38;5;28mstr\u001b[39m(MODEL_LIB_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllmodel.dll\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[0;32m---> 38\u001b[0m llmodel \u001b[38;5;241m=\u001b[39m \u001b[43mload_llmodel_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLLModelPromptContext\u001b[39;00m(ctypes\u001b[38;5;241m.\u001b[39mStructure):\n\u001b[1;32m     42\u001b[0m     _fields_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     43\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctypes\u001b[38;5;241m.\u001b[39mPOINTER(ctypes\u001b[38;5;241m.\u001b[39mc_float)),\n\u001b[1;32m     44\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctypes\u001b[38;5;241m.\u001b[39mc_size_t),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_erase\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctypes\u001b[38;5;241m.\u001b[39mc_float),\n\u001b[1;32m     57\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gpt4all/pyllmodel.py:28\u001b[0m, in \u001b[0;36mload_llmodel_library\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m ext \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdylib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinux\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mso\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindows\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdll\u001b[39m\u001b[38;5;124m\"\u001b[39m}[platform\u001b[38;5;241m.\u001b[39msystem()]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Linux, Windows, MinGW\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMODEL_LIB_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibllmodel.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdll\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /opt/conda/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllmodel.so)"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = GPT4All(model=\"/kaggle/working/orca-2-13b.Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66a6f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(llm.generate(\"The capital of France is \", max_tokens=1028))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8e100",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b596959",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.copytree('/kaggle/input/mkvc-6-finalmix/kaggle/working/mk-vectordb-chroma-six', '/kaggle/working/mk-vectordb-chroma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e887896",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"intfloat/multilingual-e5-base\", model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "vectordb = Chroma(persist_directory='mk-vectordb-chroma',embedding_function=instructor_embeddings,)\n",
    "#vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7d704",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "#retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 6, \"lambda_mult\": 0.25})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                       chain_type=\"stuff\", \n",
    "                                       retriever=retriever, \n",
    "                                       return_source_documents=True,\n",
    "                                       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029aff0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc310c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7ab46",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_ans(query):\n",
    "    llm_response = qa_chain(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d5b90",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af7c6d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/kaggle/input/mkri-qa-3/dataset.csv',sep=';')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ddf08",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for x,y in zip(dataset.no,dataset.qa_comb):\n",
    "    llm_time_start = time.time()\n",
    "    print(\"Nomor: \",x)\n",
    "    print(y)\n",
    "    llm_ans(y)\n",
    "    llm_time_end = time.time()\n",
    "    print(\"Time: \",llm_time_end - llm_time_start)\n",
    "    print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb28d6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3809209,
     "sourceId": 6602260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946908,
     "sourceId": 6867748,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 400.571098,
   "end_time": "2024-01-17T11:46:51.174705",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-17T11:40:10.603607",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
